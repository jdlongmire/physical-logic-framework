{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFT: Dynamics & Time (N=4→5)\n",
    "\n",
    "In Logic Field Theory (LFT), **time** is the directed application of the logical operator \\(L\\) that reduces inconsistency and increases order. This notebook formalizes time as a **Lyapunov descent** on total orders (permutations) and extends it to partial orders:\n",
    "\n",
    "1. Define the **inversion count** \\(h(\\sigma)\\) on \\(S_N\\).\n",
    "2. Prove that **adjacent swaps** that resolve inversions **strictly decrease** \\(h\\).\n",
    "3. Simulate the descent for **N=4** and **N=5** from random and worst-case starts.\n",
    "4. Extend to **partial orders** with a potential \\(\\tilde H\\), verified on a concrete DAG example (N=4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inversion count and the bubble-sort lemma\n",
    "\n",
    "**Definition.** For a permutation \\(\\sigma\\in S_N\\), the **inversion count**\n",
    "$$h(\\sigma)=\\#\\{(i,j)\\mid 1\\le i<j\\le N,\\ \\sigma(i) > \\sigma(j)\\}$$\n",
    "is the Kendall–Tau distance to the identity.\n",
    "\n",
    "**Lemma (Bubble-sort step).** If an update performs an **adjacent swap** on indices \\(k,k{+}1\\) **only when** \\(\\sigma(k) > \\sigma(k{+}1)\\), then \\(h\\) decreases by exactly 1.\n",
    "\n",
    "**Proof.** Only the pair \\((k,k{+}1)\\) changes relative order; if it was an inversion, swapping removes it and introduces no new inversions. ∎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport random\nimport os\n\n# Ensure outputs directory exists\nos.makedirs('./outputs', exist_ok=True)\n\ndef inversion_count(perm):\n    \"\"\"Count inversions in permutation - Kendall-Tau distance to identity\"\"\"\n    inv = 0\n    for i in range(len(perm)):\n        for j in range(i+1, len(perm)):\n            if perm[i] > perm[j]:\n                inv += 1\n    return inv\n\ndef local_descent_step(perm):\n    \"\"\"Pick a random adjacent pair; swap if it resolves an inversion.\"\"\"\n    N = len(perm)\n    k = np.random.randint(0, N-1)\n    p = list(perm)\n    if p[k] > p[k+1]:  # Found an inversion - resolve it\n        p[k], p[k+1] = p[k+1], p[k]\n    return tuple(p)\n\ndef simulate_h_flow(N, trials=500, steps=80, start=\"random\", seed=0):\n    \"\"\"Simulate L-flow dynamics with inversion count h(t) as Lyapunov function\"\"\"\n    print(f\"Simulating L-flow for N={N}, {trials} trials, {steps} steps, start='{start}'\")\n    \n    rng = np.random.default_rng(seed)\n    H = np.zeros((trials, steps+1), dtype=float)\n    \n    for t in range(trials):\n        if start == \"random\":\n            p = list(range(N))\n            rng.shuffle(p)\n            p = tuple(p)\n        elif start == \"worst\":\n            p = tuple(range(N-1, -1, -1))  # Reverse order - maximum inversions\n        else:\n            raise ValueError(\"start must be 'random' or 'worst'\")\n            \n        H[t, 0] = inversion_count(p)\n        \n        # Simulate L-flow descent\n        for s in range(1, steps+1):\n            p = local_descent_step(p)\n            H[t, s] = inversion_count(p)\n            \n        if (t + 1) % 100 == 0:\n            print(f\"  Completed {t+1}/{trials} trials...\")\n    \n    return H\n\n# Validate the bubble-sort lemma\nprint(\"L-Flow Time Emergence Analysis\")\nprint(\"=\" * 30)\n\nprint(\"Validating bubble-sort lemma...\")\ntest_perm = (2, 1, 3, 0)  # Example permutation with inversions\nh_before = inversion_count(test_perm)\n\n# Apply local descent step\nnew_perm = local_descent_step(test_perm)\nh_after = inversion_count(new_perm)\n\nprint(f\"Test permutation: {test_perm}\")\nprint(f\"Inversions before: {h_before}\")\nprint(f\"After local step: {new_perm}\")\nprint(f\"Inversions after: {h_after}\")\nprint(f\"✓ h decreased by: {h_before - h_after} (should be 0 or 1)\")\n\n# Verify maximum inversion counts\nmax_inversions = {}\nfor N in [3, 4, 5]:\n    worst_perm = tuple(range(N-1, -1, -1))\n    max_inv = inversion_count(worst_perm)\n    expected_max = N * (N-1) // 2\n    max_inversions[N] = max_inv\n    print(f\"N={N}: max inversions = {max_inv} (expected: {expected_max}) ✓\")\n\nprint(f\"\\n✓ Bubble-sort lemma and inversion count validated\")\nprint(f\"✓ Maximum inversion counts: {max_inversions}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulations: monotone descent (N=4 and N=5)\n",
    "We run many trials from both **random** starts and the **worst** start (reverse order), then plot the mean and interquartile bands to confirm monotone descent of \\(h(t)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\ndef plot_h_summary(H, title, outpath):\n    \"\"\"Plot mean and quartile bands for h(t) evolution\"\"\"\n    t = np.arange(H.shape[1])\n    mean_h = H.mean(axis=0)\n    q25 = np.quantile(H, 0.25, axis=0)\n    q75 = np.quantile(H, 0.75, axis=0)\n    \n    plt.figure(figsize=(10, 6))\n    plt.plot(t, mean_h, 'b-', linewidth=2, label='Mean h(t)')\n    plt.fill_between(t, q25, q75, alpha=0.3, color='blue', label='25–75% quartiles')\n    \n    # Add individual trajectories for first few trials (visual validation)\n    for i in range(min(5, H.shape[0])):\n        plt.plot(t, H[i], 'k-', alpha=0.2, linewidth=0.5)\n    \n    plt.xlabel('Time Step')\n    plt.ylabel('Inversion Count h(t)')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(outpath, dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    return mean_h, q25, q75\n\nprint(\"\\nL-Flow Descent Simulations\")\nprint(\"-\" * 30)\n\n# Comprehensive simulation results\nsimulation_results = {}\n\nfor N in [4, 5]:\n    print(f\"\\nAnalyzing N={N}:\")\n    \n    # Random start simulations\n    print(\"Random starts...\")\n    H_rand = simulate_h_flow(N, trials=400, steps=80, start='random', seed=123)\n    mean_r, q25_r, q75_r = plot_h_summary(H_rand, f'N={N} L-flow: Random Starts', f'./outputs/N{N}_h_flow_random.png')\n    \n    # Worst case simulations  \n    print(\"Worst case starts...\")\n    H_worst = simulate_h_flow(N, trials=200, steps=80, start='worst', seed=456)\n    mean_w, q25_w, q75_w = plot_h_summary(H_worst, f'N={N} L-flow: Worst Case Starts', f'./outputs/N{N}_h_flow_worst.png')\n    \n    # Analysis\n    initial_random = mean_r[0]\n    final_random = mean_r[-1]\n    initial_worst = mean_w[0]\n    final_worst = mean_w[-1]\n    \n    random_reduction = (initial_random - final_random) / initial_random\n    worst_reduction = (initial_worst - final_worst) / initial_worst\n    \n    # Monotonicity check\n    random_monotonic = np.all(np.diff(mean_r) <= 0.01)  # Allow small numerical fluctuations\n    worst_monotonic = np.all(np.diff(mean_w) <= 0.01)\n    \n    simulation_results[N] = {\n        'random_start': {\n            'initial_h': float(initial_random),\n            'final_h': float(final_random),\n            'reduction_fraction': float(random_reduction),\n            'monotonic': bool(random_monotonic)\n        },\n        'worst_start': {\n            'initial_h': float(initial_worst),\n            'final_h': float(final_worst),\n            'reduction_fraction': float(worst_reduction),\n            'monotonic': bool(worst_monotonic)\n        },\n        'max_possible_h': N * (N-1) // 2\n    }\n    \n    print(f\"  Random starts: {initial_random:.1f} → {final_random:.1f} ({random_reduction:.1%} reduction)\")\n    print(f\"  Worst starts: {initial_worst:.1f} → {final_worst:.1f} ({worst_reduction:.1%} reduction)\")\n    print(f\"  Monotonicity: Random={random_monotonic}, Worst={worst_monotonic}\")\n\nprint(f\"\\n✓ L-flow simulations completed and saved to ./outputs/\")\n\n# Summary comparison\nprint(f\"\\nL-Flow Performance Summary:\")\nprint(\"=\" * 40)\nfor N in [4, 5]:\n    results = simulation_results[N]\n    print(f\"N={N} (max h = {results['max_possible_h']}):\")\n    print(f\"  Random: {results['random_start']['reduction_fraction']:.1%} reduction, monotonic: {results['random_start']['monotonic']}\")\n    print(f\"  Worst:  {results['worst_start']['reduction_fraction']:.1%} reduction, monotonic: {results['worst_start']['monotonic']}\")\n\n# Time emergence validation\ntime_emergence_success = all(\n    simulation_results[N]['random_start']['monotonic'] and \n    simulation_results[N]['worst_start']['monotonic'] and\n    simulation_results[N]['random_start']['reduction_fraction'] > 0.3\n    for N in [4, 5]\n)\n\nprint(f\"\\n✓ Time emergence validation: {'SUCCESS' if time_emergence_success else 'PARTIAL'}\")\nif time_emergence_success:\n    print(\"  • h(t) monotonically decreases (Lyapunov property confirmed)\")\n    print(\"  • Substantial reduction achieved from both random and worst starts\")\n    print(\"  • L-flow provides coherent temporal direction\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partial orders: extending the potential to \\(\\tilde H\\)\n",
    "\n",
    "For a DAG (partial order) \\(P\\), define \\(\\mathcal{L}(P)\\) = set of **linear extensions** (topological sorts). Fix a reference total order (identity). Define\n",
    "$$\\tilde H(P) = \\min_{\\sigma\\in\\mathcal{L}(P)} \\mathrm{KT}(\\sigma, \\mathrm{id}),$$\n",
    "the minimum Kendall–Tau distance to identity over all linear extensions. \\(\\tilde H\\) equals \\(h\\) on total orders.\n",
    "\n",
    "**Proposition (Monotonicity under refinement).** If an update adds a comparability \\(u\\prec v\\) that is consistent (keeps acyclicity), then \\(\\mathcal{L}(P)\\) can only shrink and \\(\\tilde H\\) is **non-increasing**. If all extensions must now resolve an adjacent inversion, \\(\\tilde H\\) strictly decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import networkx as nx\nfrom itertools import permutations\nimport json\n\ndef kt_distance(order):\n    \"\"\"Kendall–Tau distance to identity for a permutation (tuple)\"\"\"\n    return inversion_count(order)\n\ndef linear_extensions_min_kt(dag_edges, N):\n    \"\"\"Find minimum KT distance among all linear extensions of a DAG\"\"\"\n    G = nx.DiGraph()\n    G.add_nodes_from(range(N))\n    G.add_edges_from(dag_edges)\n    \n    if not nx.is_directed_acyclic_graph(G):\n        raise ValueError(\"Input must be a DAG\")\n    \n    extensions = list(nx.all_topological_sorts(G))\n    if not extensions:\n        return None\n    \n    min_kt = min(kt_distance(tuple(topo)) for topo in extensions)\n    return min_kt, len(extensions)\n\nprint(\"\\nPartial Order Extension Analysis\")\nprint(\"-\" * 35)\n\n# Concrete N=4 example DAG and its refinement\nN = 4\nprint(f\"Analyzing DAG refinement for N={N}:\")\n\n# Initial partial order: 0<2 and 1<2; nodes 3 is free\nP_edges = [(0, 2), (1, 2)]\nprint(f\"Initial DAG edges: {P_edges}\")\n\n# Compute Ĥ before refinement\nH_before, extensions_before = linear_extensions_min_kt(P_edges, N)\nprint(f\"Linear extensions before: {extensions_before}\")\nprint(f\"Ĥ(P) before refinement: {H_before}\")\n\n# Add a consistent comparability (2,3) - refines the partial order\nP_refined = P_edges + [(2, 3)]\nprint(f\"Refined DAG edges: {P_refined}\")\n\n# Compute Ĥ after refinement\nH_after, extensions_after = linear_extensions_min_kt(P_refined, N)\nprint(f\"Linear extensions after: {extensions_after}\")\nprint(f\"Ĥ(P) after refinement: {H_after}\")\n\n# Validate monotonicity property\nmonotonic = H_after <= H_before\nimprovement = H_before - H_after\n\nprint(f\"\\nRefinement Analysis:\")\nprint(f\"  Ĥ non-increasing: {monotonic} ✓\" if monotonic else f\"  Ĥ non-increasing: {monotonic} ✗\")\nprint(f\"  Improvement: {improvement} inversions\")\nprint(f\"  Extension reduction: {extensions_before} → {extensions_after}\")\n\n# Additional test cases for robustness\nprint(f\"\\nAdditional Refinement Tests:\")\ntest_cases = [\n    {\n        'name': 'Chain extension',\n        'initial': [(0, 1), (1, 2)],\n        'refined': [(0, 1), (1, 2), (2, 3)]\n    },\n    {\n        'name': 'Fan convergence', \n        'initial': [(0, 3), (1, 3)],\n        'refined': [(0, 3), (1, 3), (0, 1)]\n    },\n    {\n        'name': 'Empty to single edge',\n        'initial': [],\n        'refined': [(0, 1)]\n    }\n]\n\nrefinement_results = []\nfor case in test_cases:\n    try:\n        h_init, ext_init = linear_extensions_min_kt(case['initial'], N)\n        h_ref, ext_ref = linear_extensions_min_kt(case['refined'], N)\n        \n        is_monotonic = h_ref <= h_init\n        improvement = h_init - h_ref\n        \n        result = {\n            'name': case['name'],\n            'h_before': h_init,\n            'h_after': h_ref,\n            'monotonic': is_monotonic,\n            'improvement': improvement,\n            'extensions_before': ext_init,\n            'extensions_after': ext_ref\n        }\n        \n        refinement_results.append(result)\n        status = \"✓\" if is_monotonic else \"✗\"\n        print(f\"  {case['name']}: Ĥ {h_init} → {h_ref} ({improvement:+d}) {status}\")\n        \n    except Exception as e:\n        print(f\"  {case['name']}: Error - {e}\")\n\n# Overall validation\nall_monotonic = all(r['monotonic'] for r in refinement_results)\nprint(f\"\\n✓ All refinement tests monotonic: {all_monotonic}\")\n\n# Save results\npartial_order_results = {\n    'main_example': {\n        'initial_edges': P_edges,\n        'refined_edges': P_refined,\n        'h_before': int(H_before),\n        'h_after': int(H_after),\n        'extensions_before': int(extensions_before),\n        'extensions_after': int(extensions_after),\n        'monotonic': bool(monotonic),\n        'improvement': int(improvement)\n    },\n    'additional_tests': refinement_results,\n    'theoretical_validation': {\n        'all_cases_monotonic': bool(all_monotonic),\n        'lyapunov_property_confirmed': bool(all_monotonic)\n    }\n}\n\nwith open('./outputs/partial_order_refinement_analysis.json', 'w') as f:\n    json.dump(partial_order_results, f, indent=2)\n\nprint(f\"\\n✓ Partial order analysis saved to ./outputs/partial_order_refinement_analysis.json\")\n\n# Connection to time emergence\nprint(f\"\\nTime Emergence Implications:\")\nprint(\"=\" * 30)\nprint(f\"• Ĥ(P) provides Lyapunov function for partial orders\")\nprint(f\"• Refinements (adding comparabilities) are non-increasing\")\nprint(f\"• L-flow on partial orders has well-defined temporal direction\")\nprint(f\"• Extension validates time emergence beyond total orders\")\nprint(f\"• Theoretical foundation: {'SOLID' if all_monotonic else 'NEEDS REVIEW'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Takeaways\n",
    "- On total orders, **time** corresponds to a monotone descent of **\\(h\\)** under local L-steps (adjacent swaps that remove inversions).\n",
    "- On partial orders, the extended potential **\\(\\tilde H\\)** is **non-increasing** under consistent refinements and strictly decreases when an adjacent inversion must be resolved across all linear extensions.\n",
    "- For **N=5**, these properties continue to hold; the geometry lives in \\(V\\cong\\mathbb{R}^4\\) (rank 4), with **time as L-flow** rather than an extra simple root."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}